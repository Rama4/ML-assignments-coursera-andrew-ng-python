{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45a2d2e7",
   "metadata": {},
   "source": [
    "# ex7_pca.m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77ac6389",
   "metadata": {},
   "source": [
    "# ===== Part 1: Load Example Dataset  ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15069ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Import regular expressions to process emails\n",
    "import re\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e06b2da2",
   "metadata": {},
   "source": [
    "**Principal Component Analysis**  \n",
    "In this exercise, we will use principal component analysis (PCA) to perform dimensionality reduction. we will first experiment with an example 2D dataset to get intuition on how PCA works, and then use it on a bigger dataset of 5000 face image dataset.\n",
    "\n",
    "**Example Dataset**   \n",
    "To help us understand how PCA works, we will first start with a 2D dataset which has one direction of large variation and one of smaller variation. The cell below will plot the training data, also shown in here:\n",
    "\n",
    "In this part of the exercise, we will visualize what happens when we use PCA to reduce the data from 2D to 1D. In practice, we might want to reduce data from 256 to 50 dimensions, say; but using lower dimensional data in this example allows us to visualize the algorithms better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda99e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into the variable X \n",
    "data = loadmat(os.path.join('Data', 'C:\\\\Users\\\\nomaniqbal\\\\Downloads\\\\notebook\\\\HomeworkMl\\\\mlclass-ex7-jin\\\\ex7data1.mat'))\n",
    "X = data['X']\n",
    "\n",
    "#  Visualize the example dataset\n",
    "plt.plot(X[:, 0], X[:, 1], 'bo', ms=10, mec='k', mew=1)\n",
    "plt.axis([0.5, 6.5, 2, 8])\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.grid(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "803398e6",
   "metadata": {},
   "source": [
    "# ===== Part 2: Principal Component Analysis ====="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb615dfa",
   "metadata": {},
   "source": [
    "**Implementing PCA**  \n",
    "In this part of the exercise, we will implement PCA.  \n",
    "PCA consists of two computational steps:  \n",
    "\n",
    "Compute the covariance matrix of the data.\n",
    "Use SVD (in python we use numpy's implementation np.linalg.svd) to compute the eigenvectors $U_1, U_2, U_n$ These will correspond to the principal components of variation in the data.\n",
    "First, we should compute the covariance matrix of the data, which is given by:\n",
    "$\\Sigma = \\frac{1}{m} X^T X$\n",
    "\n",
    "where $X$ is the data matrix with examples in rows, and $m$ is the number of examples. Note that $\\Sigma$ is a $n \\times n$ matrix and not the summation operator.\n",
    "\n",
    "After computing the covariance matrix, we can run SVD on it to compute the principal components. In python and numpy (or scipy), we can run SVD with the following command: U, S, V = np.linalg.svd(Sigma), where U will contain the principal components and S will contain a diagonal matrix. Note that the scipy library also has a similar function to compute SVD scipy.linalg.svd. The functions in the two libraries use the same C-based library (LAPACK) for the SVD computation, but the scipy version provides more options and arguments to control SVD computation. In this exercise, we will stick with the numpy implementation of SVD.\n",
    "\n",
    "Complete the code in the following cell to implemente PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d829916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X):\n",
    "\n",
    "    # Useful values\n",
    "    m, n = X.shape\n",
    "\n",
    "    # We need to return the following variables correctly.\n",
    "    U = np.zeros(n)\n",
    "    S = np.zeros(n)\n",
    "\n",
    "    Sigma = (1 / m) * (X.T.dot(X))\n",
    "    U, S, V = np.linalg.svd(Sigma)\n",
    "    \n",
    "    return U, S"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09a1d48e",
   "metadata": {},
   "source": [
    "Before using PCA, it is important to first normalize the data by subtracting the mean value of each feature from the dataset, and scaling each dimension so that they are in the same range.\n",
    "\n",
    "In the next cell, this normalization will be performed for us using the feature_normalization function. After normalizing the data, we can run PCA to compute the principal components. our task is to complete the code in the function pca to compute the principal components of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aef06dd8",
   "metadata": {},
   "source": [
    "The following cell will also output the top principal component (eigenvector) found, and we should expect to see an output of about [-0.707 -0.707]. (It is possible that numpy may instead output the negative of this, since U1 and -U1 are equally valid choices for the first principal component.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56debf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalization(X):\n",
    "\n",
    "    standard_deviation = np.std(X, axis= 0)\n",
    "    mean = np.mean(X, axis= 0)\n",
    "    X_normalized = (X - mean)/ standard_deviation\n",
    "    \n",
    "    return X_normalized, mean, standard_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Before running PCA, it is important to first normalize X\n",
    "X_norm, mu, sigma = feature_normalization(X)\n",
    "\n",
    "#  Run PCA\n",
    "U, S = pca(X_norm)\n",
    "\n",
    "#  Draw the eigenvectors centered at mean of data. These lines show the\n",
    "#  directions of maximum variations in the dataset.\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X[:, 0], X[:, 1], 'bo', ms=10, mec='k', mew=0.25)\n",
    "\n",
    "for i in range(2):\n",
    "    ax.arrow(mu[0], mu[1], 1.5 * S[i]*U[0, i], 1.5 * S[i]*U[1, i],\n",
    "             head_width=0.25, head_length=0.2, fc='k', ec='k', lw=2, zorder=1000)\n",
    "\n",
    "ax.axis([0.5, 6.5, 2, 8])\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(False)\n",
    "\n",
    "print('Top eigenvector: U[:, 0] = [{:.6f} {:.6f}]'.format(U[0, 0], U[1, 0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0f02a01",
   "metadata": {},
   "source": [
    "# ===== Part 3: Dimension Reduction ====="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92247543",
   "metadata": {},
   "source": [
    "**Dimensionality Reduction with PCA**  \n",
    "After computing the principal components, we can use them to reduce the feature dimension of our dataset by projecting each example onto a lower dimensional space, $x^{(i)} \\rightarrow z^{(i)}$ (e.g., projecting the data from 2D to 1D). In this part of the exercise, we will use the eigenvectors returned by PCA and project the example dataset into a 1-dimensional space. In practice, if we were using a learning algorithm such as linear regression or perhaps neural networks, we could now use the projected data instead of the original data. By using the projected data, we can train our model faster as there are less dimensions in the input."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e780d81a",
   "metadata": {},
   "source": [
    "**Projecting the data onto the principal components**  \n",
    "we should now complete the code in the function project_data. Specifically, we are given a dataset X, the principal components U, and the desired number of dimensions to reduce to K. we should project each example in X onto the top K components in U. Note that the top K components in U are given by the first K columns of U, that is Ureduce = U[:, :K]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3220af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_data(X, U, K):\n",
    "\n",
    "    # we need to return the following variables correctly.\n",
    "    Z = np.zeros((X.shape[0], K))\n",
    "\n",
    "    Z = np.dot(X,  U[:, :K])\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c21e3abf",
   "metadata": {},
   "source": [
    "Once we have completed the code in project_data, the following cell will project the first example onto the first dimension and we should see a value of about 1.481 (or possibly -1.481, if we got -U1 instead of U)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6268ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Project the data onto K = 1 dimension\n",
    "K = 1\n",
    "Z = project_data(X_norm, U, K)\n",
    "print('Projection of the first example: {:.6f}'.format(Z[0, 0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "379e3729",
   "metadata": {},
   "source": [
    "**Reconstructing an approximation of the data**  \n",
    "After projecting the data onto the lower dimensional space, we can approximately recover the data by projecting them back onto the original high dimensional space. Our task is to complete the function recover_data to project each example in Z back onto the original space and return the recovered approximation in Xrec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_data(Z, U, K):\n",
    "\n",
    "    # We need to return the following variables correctly.\n",
    "    X_rec = np.zeros((Z.shape[0], U.shape[0]))\n",
    "\n",
    "    X_rec = Z.dot(U[:, :K].T)\n",
    "\n",
    "    return X_rec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49eafa26",
   "metadata": {},
   "source": [
    "Once we have completed the code in recover_data, a cell will recover an approximation of the first example and we should see a value of about [-1.047 -1.047]. The code will then plot the data in this reduced dimension space. This will show us what the data looks like when using only the corresponding eigenvectors to reconstruct it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb81294",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rec  = recover_data(Z, U, K)\n",
    "print('Approximation of the first example: [{:.6f} {:.6f}]'.format(X_rec[0, 0], X_rec[0, 1]))\n",
    "print('       (this value should be about  [-1.047419 -1.047419])')\n",
    "\n",
    "#  Plot the normalized dataset (returned from featureNormalize)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(X_norm[:, 0], X_norm[:, 1], 'bo', ms=8, mec='b', mew=0.5)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(False)\n",
    "plt.axis([-3, 2.75, -3, 2.75])\n",
    "\n",
    "# Draw lines connecting the projected points to the original points\n",
    "ax.plot(X_rec[:, 0], X_rec[:, 1], 'ro', mec='r', mew=2, mfc='none')\n",
    "for xnorm, xrec in zip(X_norm, X_rec):\n",
    "    ax.plot([xnorm[0], xrec[0]], [xnorm[1], xrec[1]], '--k', lw=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b51385d5",
   "metadata": {},
   "source": [
    "# ===== Part 4: Loading and Visualizing Face Data ====="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01ecd70d",
   "metadata": {},
   "source": [
    "In this part of the exercise, we will run PCA on face images to see how it can be used in practice for dimension reduction. The dataset ex7faces.mat contains a dataset X of face images, each  in grayscale. This dataset was based on a cropped version of the labeled faces in the wild dataset. Each row of X corresponds to one face image (a row vector of length 1024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcaf17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_data(X, example_width=None, figsize=(10, 10)):\n",
    "\n",
    "    # Compute rows, cols\n",
    "    if X.ndim == 2:\n",
    "        m, n = X.shape\n",
    "    elif X.ndim == 1:\n",
    "        n = X.size\n",
    "        m = 1\n",
    "        X = X[None]  # Promote to a 2 dimensional array\n",
    "    else:\n",
    "        raise IndexError('Input X should be 1 or 2 dimensional.')\n",
    "\n",
    "    example_width = example_width or int(np.round(np.sqrt(n)))\n",
    "    example_height = int(n / example_width)\n",
    "\n",
    "    # Compute number of items to display\n",
    "    display_rows = int(np.floor(np.sqrt(m)))\n",
    "    display_cols = int(np.ceil(m / display_rows))\n",
    "\n",
    "    fig, ax_array = plt.subplots(display_rows, display_cols, figsize=figsize)\n",
    "    fig.subplots_adjust(wspace=0.025, hspace=0.025)\n",
    "\n",
    "    ax_array = [ax_array] if m == 1 else ax_array.ravel()\n",
    "\n",
    "    for i, ax in enumerate(ax_array):\n",
    "        ax.imshow(X[i].reshape(example_height, example_width, order='F'), cmap='gray')\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3dd7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Face dataset\n",
    "data = loadmat(\"C:\\\\Users\\\\nomaniqbal\\\\Downloads\\\\notebook\\\\HomeworkMl\\\\mlclass-ex7-jin\\\\ex7faces.mat\")\n",
    "X = data['X']\n",
    "\n",
    "#  Display the first 100 faces in the dataset\n",
    "display_data(X[:100, :], figsize=(8, 8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2874b073",
   "metadata": {},
   "source": [
    "# ===== Part 5: PCA on Face Data: Eigenfaces ====="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f90b2e42",
   "metadata": {},
   "source": [
    "To run PCA on the face dataset, we first normalize the dataset by subtracting the mean of each feature from the data matrix X. After running PCA, we will obtain the principal components of the dataset. Notice that each principal component in U (each column) is a vector of length  (where for the face dataset, n=1024 ). It turns out that we can visualize these principal components by reshaping each of them into a 32*32 matrix that corresponds to the pixels in the original dataset.\n",
    "\n",
    "The following cell will first normalize the dataset for we and then run our PCA code. Then, the first 36 principal components (conveniently called eigenfaces) that describe the largest variations are displayed. If we want, we can also change the code to display more principal components to see how they capture more and more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79331094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  normalize X by subtracting the mean value from each feature\n",
    "X_norm, mu, sigma = feature_normalization(X)\n",
    "\n",
    "#  Run PCA\n",
    "U, S = pca(X_norm)\n",
    "\n",
    "#  Visualize the top 36 eigenvectors found\n",
    "display_data(U[:, :36].T, figsize=(8, 8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe10aa32",
   "metadata": {},
   "source": [
    "# ===== Part 6: Dimension Reduction for Faces ====="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f8d48d8",
   "metadata": {},
   "source": [
    "Now that we have computed the principal components for the face dataset, we can use it to reduce the dimension of the face dataset. This allows us to use our learning algorithm with a smaller input size (e.g., 100 dimensions) instead of the original 1024 dimensions. This can help speed up our learning algorithm.\n",
    "\n",
    "The next cell will project the face dataset onto only the first 100 principal components. Concretely, each face image is now described by a vector $z^{(i)} \\in \\mathbb{R}^{100}$. To understand what is lost in the dimension reduction, we can recover the data using only the projected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c78557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Project images to the eigen space using the top k eigenvectors \n",
    "#  If we are applying a machine learning algorithm \n",
    "K = 100\n",
    "Z = project_data(X_norm, U, K)\n",
    "\n",
    "print('The projected data Z has a shape of: ', Z.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8610aaf8",
   "metadata": {},
   "source": [
    "**In the next cell, an approximate recovery of the data is performed and the original and projected face images are displayed**\n",
    "\n",
    "From the reconstruction, we can observe that the general structure and appearance of the face are kept while the fine details are lost. This is a remarkable reduction (more than 10x) in the dataset size that can help speed up our learning algorithm significantly. For example, if we were training a neural network to perform person recognition (given a face image, predict the identity of the person), we can use the dimension reduced input of only a 100 dimensions instead of the original pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da6fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Project images to the eigen space using the top K eigen vectors and \n",
    "#  visualize only using those K dimensions\n",
    "#  Compare to the original input, which is also displayed\n",
    "K = 100\n",
    "X_rec  = recover_data(Z, U, K)\n",
    "\n",
    "# Display normalized data\n",
    "display_data(X_norm[:100, :], figsize=(6, 6))\n",
    "plt.gcf().suptitle('Original faces')\n",
    "\n",
    "# Display reconstructed data from only k eigenfaces\n",
    "display_data(X_rec[:100, :], figsize=(6, 6))\n",
    "plt.gcf().suptitle('Recovered faces')\n",
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dade5c8b",
   "metadata": {},
   "source": [
    "# ===== Part 7: Visualization of Faces after PCA Dimension Reduction ====="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a10acb14",
   "metadata": {},
   "source": [
    "In the earlier K-means image compression exercise, we used the K-means algorithm in the 3-dimensional RGB space. We reduced each pixel of the RGB image to be represented by 16 clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43e6fa83",
   "metadata": {},
   "source": [
    "In the next cell,we will apply our implementation of PCA to the 3-dimensional data to reduce it to 2 dimensions and visualize the result in a 2D scatter plot. The PCA projection can be thought of as a rotation that selects the view that maximizes the spread of the data, which often corresponds to the “best” view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the mean to use PCA\n",
    "X_norm, mu, sigma = feature_normalization(X)\n",
    "\n",
    "# PCA and project the data to 2D\n",
    "U, S = pca(X_norm)\n",
    "Z = project_data(X_norm, U, 2)\n",
    "\n",
    "# Reset matplotlib to non-interactive\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(Z[sel, 0], Z[sel, 1], cmap='rainbow', c=idx[sel], s=64)\n",
    "ax.set_title('Pixel dataset plotted in 2D, using PCA for dimensionality reduction')\n",
    "ax.grid(False)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae85eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
