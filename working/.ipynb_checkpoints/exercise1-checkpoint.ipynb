{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f9b71b",
   "metadata": {},
   "source": [
    "# Exercise 1: Linear Regression\n",
    "\n",
    "In the first part of Exercise 1, we have been provided with a warmUpExercise in which we will be printing a 5*5 identity matrix and in the second part we have been provided with a dataset consisting of two columns. The first column is the population of a city and the second column is the profit of a food truck in that city. Remember, a negative value for profit indicates a loss.\n",
    "\n",
    "we will solve the exercise step by step.  \n",
    "\n",
    "before getting started, lets import all the important packages that we will need later on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b070ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the important Libraries\n",
    "\n",
    "# The OS module in Python provides functions for interacting with the operating system.\n",
    "# Used for manipulating directory path.s\n",
    "import os\n",
    "\n",
    "# Used for computations of numerical data.\n",
    "import numpy as np\n",
    "\n",
    "# Used for reading data and data manipulation\n",
    "import pandas as pd \n",
    "\n",
    "# Used for graphing data.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# needed to plot 3-D surfaces\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48001d",
   "metadata": {},
   "source": [
    "# ==================== Part 1: Basic Function ====================\n",
    "### warmUpExercise.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89815d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmUpExercise():  \n",
    "    A = np.eye(5)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78925c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmUpExercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b095894",
   "metadata": {},
   "source": [
    "# ======================= Part 2: Plotting =======================\n",
    "\n",
    "### ex1data1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f97e2f5",
   "metadata": {},
   "source": [
    "### * Linear regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0683163",
   "metadata": {},
   "source": [
    "### Reading and Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ed96e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6.1101</th>\n",
       "      <th>17.592</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.13020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.66200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.85400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.82330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.3829</td>\n",
       "      <td>11.88600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>5.8707</td>\n",
       "      <td>7.20290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>5.3054</td>\n",
       "      <td>1.98690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>8.2934</td>\n",
       "      <td>0.14454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>13.3940</td>\n",
       "      <td>9.05510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.4369</td>\n",
       "      <td>0.61705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     6.1101    17.592\n",
       "0    5.5277   9.13020\n",
       "1    8.5186  13.66200\n",
       "2    7.0032  11.85400\n",
       "3    5.8598   6.82330\n",
       "4    8.3829  11.88600\n",
       "..      ...       ...\n",
       "91   5.8707   7.20290\n",
       "92   5.3054   1.98690\n",
       "93   8.2934   0.14454\n",
       "94  13.3940   9.05510\n",
       "95   5.4369   0.61705\n",
       "\n",
       "[96 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'../DataSets/Exercise-1/ex1data1.txt', sep=\",\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c9c0625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['6.1101', '17.592'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29d46a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rama4\\AppData\\Local\\Temp\\ipykernel_15876\\338400415.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset=pd.DataFrame(np.array([[6.1101, 17.592]]), columns=['Population(10k)','Profit($ 10k)']).append(dataset, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Population(10k)', 'Profit($ 10k)'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming columns\n",
    "dataset.rename(columns = {'6.1101':'Population(10k)', '17.592':'Profit($ 10k)'}, inplace = True)\n",
    "dataset=pd.DataFrame(np.array([[6.1101, 17.592]]), columns=['Population(10k)','Profit($ 10k)']).concat(dataset, ignore_index=True)\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b607b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5842b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,0].values\n",
    "y = dataset.iloc[:,1].values\n",
    "print(\"X=\", X.size)\n",
    "print(X)\n",
    "m = y.size # no. of training example (labels)\n",
    "print(\"Y=\", m)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c5c1d",
   "metadata": {},
   "source": [
    "### Plotting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa948a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "\n",
    "# 'ro' option refers to the markers(red circles), 'ms' refers to marker size while 'mec' property refers to the marker edge color.\n",
    "plt.plot(X, y, 'ro', ms=10, mec='b')\n",
    "plt.ylabel('Profit in $10,000')\n",
    "plt.xlabel('Population of City in 10,000s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9872469",
   "metadata": {},
   "source": [
    "# =================== Part 3: Gradient descent ===================\n",
    "### Gradient Descent\n",
    "As we have learned from the course that Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost) and the cost function or Sum of Squeared Errors(SSE) is a measure of how far away our hypothesis is from the optimal hypothesis. \n",
    "\n",
    "In this part, we will fit the linear regression parameters $θ$ to our dataset using gradient descent.\n",
    "\n",
    "The objective of linear regression is to minimize the cost function  \n",
    "\n",
    "$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$  \n",
    "\n",
    "where the hypothesis $h_\\theta(x)$ is given by the linear model  \n",
    "$h_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1 x_1$  \n",
    "\n",
    "Recall that the parameters of our model are the $\\theta_j$ values. These are the values we will adjust to minimize cost $J(\\theta)$. One way to do this is to use the batch gradient descent algorithm. In batch gradient descent, each iteration performs the update  \n",
    "$\\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} \\qquad \\text{simultaneously update } \\theta_j \\text{ for all } j$\n",
    "\n",
    "With each step of gradient descent, our parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost J($\\theta$).,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbcccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the intercept\n",
    "# X and Y are rank 1 arrays. rank 1 array's shape = (m, ) where as rank 2 array's shape = (m,1). \n",
    "# When operating on arrays its good to convert rank 1 arrays to rank 2 arrays because rank 1 arrays often give unexpected \n",
    "# results. To convert rank 1 to rank 2 array we use someArray[:,np.newaxis].\n",
    "X = X[:, np.newaxis]\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "#initializing theta value into 0\n",
    "theta = np.zeros([2,1])\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "ones = np.ones((m,1))\n",
    "X = np.hstack((ones, X)) # adding the intercept term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c82662",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57280dd",
   "metadata": {},
   "source": [
    "# Computing the cost $J(\\theta)$\n",
    "\n",
    "Now we will be computing the cost and the gradient descent. First we will compute the cost function and then find optimal parameters using gradient descent.\n",
    "\n",
    "As we perform gradient descent to learn minimize the cost function $J(\\theta)$, it is helpful to monitor the convergence by computing the cost. In this section, we will implement a function to calculate $J(\\theta)$ so we can check the convergence of our gradient descent implementation.\n",
    "\n",
    "our next task is to complete the code for the function computeCost using gradient descent which computes $J(\\theta)$. As we are doing this, remember that the variables  X and y  are not scalar values. X is a matrix whose rows represent the examples from the training set and y is a vector whose each elemennt represent the value at a given row of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d74463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "def computeCost(X, y, theta):\n",
    "    temp = np.dot(X, theta) - y\n",
    "    return np.sum(np.power(temp, 2)) / (2*m)\n",
    "J = computeCost(X, y, theta)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6dd36e",
   "metadata": {},
   "source": [
    "# Finding the optimal parameters using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, iterations):\n",
    "    for _ in range(iterations):\n",
    "        temp = np.dot(X, theta) - y\n",
    "        temp = np.dot(X.T, temp)\n",
    "        theta = theta - (alpha/m) * temp\n",
    "    return theta\n",
    "theta = gradientDescent(X, y, theta, alpha, iterations)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec92c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have the optimized value of theta, lets Use this value in the above cost function and compare both the values.\n",
    "J = computeCost(X, y, theta)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad7794",
   "metadata": {},
   "source": [
    "We got a value of 4.483 which is much better than 32.07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bffe682",
   "metadata": {},
   "source": [
    "# Plot showing the best fit line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f3ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], y)\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.plot(X[:,1], np.dot(X, theta))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e2229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values for population sizes of 35,000 and 70,000\n",
    "predict1 = np.dot([1, 3.5], theta)\n",
    "print('For population size of 35,000 profit =', predict1*10000)\n",
    "\n",
    "predict2 = np.dot([1, 7], theta)\n",
    "print('For population size of 70,000 profit =', predict2*10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b19a4",
   "metadata": {},
   "source": [
    "# ============= Part 4: Visualizing J(theta_0, theta_1) =============\n",
    "\n",
    "###  Visualizing J(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf639efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid over which we will calculate J\n",
    "theta0_vals = np.linspace(-10, 10, 100)\n",
    "theta1_vals = np.linspace(-1, 4, 100)\n",
    "\n",
    "# initialize J_vals to a matrix of 0's\n",
    "J_vals = np.zeros((theta0_vals.shape[0], theta1_vals.shape[0]))\n",
    "\n",
    "# Fill out J_vals\n",
    "for i, theta0 in enumerate(theta0_vals):\n",
    "    for j, theta1 in enumerate(theta1_vals):\n",
    "        J_vals[i, j] = computeCost(X, y, [theta0, theta1])\n",
    "        \n",
    "# Because of the way meshgrids work in the surf command, we need to\n",
    "# transpose J_vals before calling surf, or else the axes will be flipped\n",
    "J_vals = J_vals.T\n",
    "\n",
    "# surface plot\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap='viridis')\n",
    "plt.xlabel('theta0')\n",
    "plt.ylabel('theta1')\n",
    "plt.title('Surface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f477c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contour plot\n",
    "# Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = plt.subplot(122)\n",
    "plt.contour(theta0_vals, theta1_vals, J_vals, linewidths=2, cmap='viridis', levels=np.logspace(-2, 3, 20))\n",
    "plt.xlabel('theta0')\n",
    "plt.ylabel('theta1')\n",
    "plt.plot(theta[0], theta[1], 'ro', ms=10, lw=2)\n",
    "plt.title('Contour, showing minimum')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e87462",
   "metadata": {},
   "source": [
    "# * Linear regression with multiple variables\n",
    "\n",
    "### ex1data2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb51b0",
   "metadata": {},
   "source": [
    "Now, we will implement linear regression with multiple variables (also called Multivariate Linear Regression).\n",
    "The file ex1data2.txt contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ff19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv(r'C:\\Users\\nomaniqbal\\Downloads\\notebook\\HomeworkMl\\mlclass-ex1-jin\\ex1data2.txt', sep = ',', header = None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732252f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,0:2] # read first two columns into X\n",
    "y = data.iloc[:,2] # read the third column into y\n",
    "m = len(y) # no. of training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffddc506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "data.rename(columns = {0:'size', 1:'bedrooms', 2:'price'}, inplace = True)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac6a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadae109",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064969bb",
   "metadata": {},
   "source": [
    "# ================ Part 1: Feature Normalization ================\n",
    "### Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eedfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the mean value of each feature from the dataset.\n",
    "# After subtracting the mean, additionally scale (divide) the feature values by their respective “standard deviations.”\n",
    "X = (X - np.mean(X))/np.std(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29610e1e",
   "metadata": {},
   "source": [
    "### Adding the intercept term and initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones((m,1))\n",
    "X = np.hstack((ones, X))\n",
    "alpha = 0.01\n",
    "num_iters = 400\n",
    "theta = np.zeros((3,1))\n",
    "y = y[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fbdb98",
   "metadata": {},
   "source": [
    "# ================ Part 2: Gradient Descent ================\n",
    "\n",
    "### Computing the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c688dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCostMulti(X, y, theta):\n",
    "    temp = np.dot(X, theta) - y\n",
    "    return np.sum(np.power(temp, 2)) / (2*m)\n",
    "J = computeCostMulti(X, y, theta)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a993d",
   "metadata": {},
   "source": [
    "### Finding the optimal parameters using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68795a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentMulti(X, y, theta, alpha, iterations):\n",
    "    m = len(y)\n",
    "    for _ in range(iterations):\n",
    "        temp = np.dot(X, theta) - y\n",
    "        temp = np.dot(X.T, temp)\n",
    "        theta = theta - (alpha/m) * temp\n",
    "    return theta\n",
    "theta = gradientDescentMulti(X, y, theta, alpha, num_iters)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836b9e1",
   "metadata": {},
   "source": [
    "since we got the optimized value of theta . we will use this value in the above cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ce1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = computeCostMulti(X, y, theta)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd80b2fb",
   "metadata": {},
   "source": [
    "This this give us the value of 2105448288.6292474 which is much better than 65591548106.45744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ebde76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing and plotting model\n",
    "plot = \"bedrooms\"\n",
    "plt.scatter(data[plot], data[\"price\"])\n",
    "plt.xlabel(plot)\n",
    "plt.ylabel(\"Prices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134c1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
