{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f5fc74",
   "metadata": {},
   "source": [
    "# Programming Exercise 6: Support Vector Machines-Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3066d19",
   "metadata": {},
   "source": [
    "Many email services today provide spam filters that are able to classify emails into spam and non-spam email with high accuracy. In this part of the exercise, we will use SVMs to build our own spam filter.\n",
    "\n",
    "we will be training a classifier to classify whether a given email, x, is spam (y=1) or non-spam (y=0). In particular, we need to convert each email into a feature vector $x \\in \\mathbb{R}^n$. The following parts of the exercise will walk us through how such a feature vector can be constructed from an email.\n",
    "\n",
    "The dataset included for this exercise is based on a a subset of the SpamAssassin Public Corpus. For the purpose of this exercise, we will only be using the body of the email (excluding the email headers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49399442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing important libraries\n",
    "\n",
    "# Used for computations of numerical data.\n",
    "import numpy as np\n",
    "\n",
    "# Used for reading data and data manipulation\n",
    "import pandas as pd \n",
    "\n",
    "# Used for graphing data.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Used to load the OCTAVE *.mat files\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Optimization module in scipy\n",
    "import scipy.io \n",
    "\n",
    "# regular expression for e-mail processing\n",
    "import re \n",
    "\n",
    "# This is done to suppress tonnes of warning messages that the call to sklearn fit method generates\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# SVM software\n",
    "from sklearn import svm \n",
    "\n",
    "\n",
    "# This is one possible porter stemmer \n",
    "# (note: I had to do a pip install stemming)\n",
    "# https://pypi.python.org/pypi/stemming/1.0\n",
    "from stemming.porter2 import stem\n",
    "\n",
    "# This porter stemmer seems to more accurately duplicate the\n",
    "# porter stemmer used in the OCTAVE assignment code\n",
    "# (note: I had to do a pip install nltk)\n",
    "# I'll note that both stemmers have very similar results\n",
    "import nltk, nltk.stem.porter\n",
    "    \n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09423594",
   "metadata": {},
   "source": [
    "# ===== Part 1: Email Preprocessing ====="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f798cdd1",
   "metadata": {},
   "source": [
    "### Preprocessing Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b680f56",
   "metadata": {},
   "source": [
    "Before starting on a machine learning task, it is usually insightful to take a look at examples from the dataset. The text below shows a sample email that contains a URL, an email address (at the end), numbers, and dollar amounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55f7376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emailSample1.txt:\n",
      "> Anyone knows how much it costs to host a web portal ?\n",
      "\n",
      ">\n",
      "\n",
      "Well, it depends on how many visitors you're expecting.\n",
      "\n",
      "This can be anywhere from less than 10 bucks a month to a couple of $100. \n",
      "\n",
      "You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \n",
      "\n",
      "if youre running something big..\n",
      "\n",
      "\n",
      "\n",
      "To unsubscribe yourself from this mailing list, send an email to:\n",
      "\n",
      "groupname-unsubscribe@egroups.com\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"emailSample1.txt:\")\n",
    "path1 = (\"C:\\\\Users\\\\nomaniqbal\\\\Downloads\\\\notebook\\\\HomeworkMl\\\\mlclass-ex6-jin\\\\emailSample1.txt\")\n",
    "my_file = open(path1, \"r\")\n",
    "for line in my_file:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c6ed45",
   "metadata": {},
   "source": [
    "While many emails would contain similar types of entities (e.g., numbers, other URLs, or other email addresses), the specific entities (e.g., the specific URL or specific dollar amount) will be different in almost every email. Therefore, one method often employed in processing emails is to “normalize” these values, so that all URLs are treated the same, all numbers are treated the same, etc. For example, we could replace each URL in the email with the unique string “httpaddr” to indicate that a URL was present.\n",
    "\n",
    "This has the effect of letting the spam classifier make a classification decision based on whether any URL was present, rather than whether a specific URL was present. This typically improves the performance of a spam classifier, since spammers often randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small.\n",
    "\n",
    "In the function process_email below, we will implement the following email preprocessing and normalization steps:\n",
    "\n",
    "**Lower-casing:** The entire email is converted into lower case, so that captialization is ignored (e.g., IndIcaTE is treated the same as Indicate).\n",
    "\n",
    "**Stripping HTML:** All HTML tags are removed from the emails. Many emails often come with HTML formatting; we remove all the HTML tags, so that only the content remains.\n",
    "\n",
    "**Normalizing URLs:** All URLs are replaced with the text “httpaddr”.\n",
    "\n",
    "**Normalizing Email Addresses:** All email addresses are replaced with the text “emailaddr”.\n",
    "\n",
    "**Normalizing Numbers:** All numbers are replaced with the text “number”.\n",
    "\n",
    "**Normalizing Dollars:** All dollar signs ($) are replaced with the text “dollar”.\n",
    "\n",
    "**Word Stemming:** Words are reduced to their stemmed form. For example, “discount”, “discounts”, “discounted” and “discounting” are all replaced with “discount”. Sometimes, the Stemmer actually strips off additional characters from the end, so “include”, “includes”, “included”, and “including” are all replaced with “includ”.\n",
    "\n",
    "**Removal of non-words:** Non-words and punctuation have been removed. All white spaces (tabs, newlines, spaces) have all been trimmed to a single space character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53794019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess( email ):\n",
    "    \"\"\"\n",
    "    Function to do some pre processing (simplification of e-mails).\n",
    "    Comments throughout implementation describe what it does.\n",
    "    Input = raw e-mail\n",
    "    Output = processed (simplified) email\n",
    "    \"\"\"\n",
    "    # Make the entire e-mail lower case\n",
    "    email = email.lower()\n",
    "    \n",
    "    # Strip html tags (strings that look like <blah> where 'blah' does not\n",
    "    # contain '<' or '>')... replace with a space\n",
    "    email = re.sub('<[^<>]+>', ' ', email);\n",
    "    \n",
    "    #Any numbers get replaced with the string 'number'\n",
    "    email = re.sub('[0-9]+', 'number', email)\n",
    "    \n",
    "    #Anything starting with http or https:// replaced with 'httpaddr'\n",
    "    email = re.sub('(http|https)://[^\\s]*', 'httpaddr', email)\n",
    "    \n",
    "    #Strings with \"@\" in the middle are considered emails --> 'emailaddr'\n",
    "    email = re.sub('[^\\s]+@[^\\s]+', 'emailaddr', email);\n",
    "    \n",
    "    #The '$' sign gets replaced with 'dollar'\n",
    "    email = re.sub('[$]+', 'dollar', email);\n",
    "    \n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8164f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email2TokenList( raw_email ):\n",
    "    \"\"\"\n",
    "    Function that takes in preprocessed (simplified) email, tokenizes it,\n",
    "    stems each word, and returns an (ordered) list of tokens in the e-mail\n",
    "    \"\"\"\n",
    "    \n",
    "    # I'll use the NLTK stemmer because it more accurately duplicates the\n",
    "    # performance of the OCTAVE implementation in the assignment\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    \n",
    "    email = preProcess( raw_email )\n",
    "\n",
    "    #Split the e-mail into individual words (tokens) (split by the delimiter ' ')\n",
    "    #but also split by delimiters '@', '$', '/', etc etc\n",
    "    #Splitting by many delimiters is easiest with re.split()\n",
    "    tokens = re.split('[ \\@\\$\\/\\#\\.\\-\\:\\&\\*\\+\\=\\[\\]\\?\\!\\(\\)\\{\\}\\,\\'\\\"\\>\\_\\<\\;\\%]', email)\n",
    "    \n",
    "    #Loop over each word (token) and use a stemmer to shorten it,\n",
    "    #then check if the word is in the vocab_list... if it is,\n",
    "    #store what index in the vocab_list the word is\n",
    "    tokenlist = []\n",
    "    for token in tokens:\n",
    "        \n",
    "        #Remove any non alphanumeric characters\n",
    "        token = re.sub('[^a-zA-Z0-9]', '', token);\n",
    "\n",
    "        #Use the Porter stemmer to stem the word\n",
    "        stemmed = stemmer.stem( token )\n",
    "        \n",
    "        #Throw out empty tokens\n",
    "        if not len(token): continue\n",
    "            \n",
    "        #Store a list of all unique stemmed words\n",
    "        tokenlist.append(stemmed)\n",
    "            \n",
    "    return tokenlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b57657c",
   "metadata": {},
   "source": [
    "### Vocabulary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "184fea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = (\"C:\\\\Users\\\\nomaniqbal\\\\Downloads\\\\notebook\\\\HomeworkMl\\\\mlclass-ex6-jin\\\\vocab.txt\")\n",
    "my_file = open(path2, \"r\")\n",
    "for line in my_file:\n",
    "        line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aee2749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabDict(reverse=False):\n",
    "    \"\"\"\n",
    "    Function to read in the supplied vocab list text file into a dictionary.\n",
    "    I'll use this for now, but since I'm using a slightly different stemmer,\n",
    "    I'd like to generate this list myself from some sort of data set...\n",
    "    Dictionary key is the stemmed word, value is the index in the text file\n",
    "    If \"reverse\", the keys and values are switched.\n",
    "    \"\"\"\n",
    "    vocab_dict = {}\n",
    "    with open(path2) as f:\n",
    "        for line in f:\n",
    "            (val, key) = line.split()\n",
    "            if not reverse:\n",
    "                vocab_dict[key] = int(val)\n",
    "            else:\n",
    "                vocab_dict[int(val)] = key\n",
    "                \n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b6c5e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email2VocabIndices( raw_email, vocab_dict ):\n",
    "    \"\"\"\n",
    "    Function that takes in a raw email and returns a list of indices corresponding\n",
    "    to the location in vocab_dict for each stemmed word in the email.\n",
    "    \"\"\"\n",
    "    tokenlist = email2TokenList( raw_email )\n",
    "    index_list = [ vocab_dict[token] for token in tokenlist if token in vocab_dict ]\n",
    "    return index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f28459",
   "metadata": {},
   "source": [
    "# ===== Part 2: Feature Extraction ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a06bea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email2FeatureVector( raw_email, vocab_dict ):\n",
    "    \"\"\"\n",
    "    Function that takes as input a raw email, and returns a vector of shape\n",
    "    (n,1) where n is the size of the vocab_dict.\n",
    "    The first element in this vector is 1 if the vocab word with index == 1\n",
    "    is in the raw_email, 0 otherwise.\n",
    "    \"\"\"\n",
    "    n = len(vocab_dict)\n",
    "    result = np.zeros((n,1))\n",
    "    vocab_indices = email2VocabIndices( email_contents, vocab_dict )\n",
    "    for idx in vocab_indices:\n",
    "        result[idx] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa257d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature vector is 1899\n",
      "Number of non-zero entries is: 45\n"
     ]
    }
   ],
   "source": [
    "# \" ... running my code on the email sample. we should see that the feature vector \n",
    "# has length 1899 and 45 non-zero entries.\"\n",
    "\n",
    "vocab_dict = getVocabDict()\n",
    "email_contents = open( path1, 'r' ).read()\n",
    "test_fv = email2FeatureVector( email_contents, vocab_dict )\n",
    "\n",
    "print (\"Length of feature vector is %d\" % len(test_fv))\n",
    "print (\"Number of non-zero entries is: %d\" % sum(test_fv==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f228a47f",
   "metadata": {},
   "source": [
    "# ===== Part 3: Train Linear SVM for Spam Classification ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "363a20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training set and test set provided\n",
    "# Note the feature vectors correspond to the stemming implementation\n",
    "# done in the OCTAVE code... which may be different than mine.\n",
    "\n",
    "# Training set\n",
    "datafile = loadmat(\"C:\\\\Users\\\\nomaniqbal\\\\Downloads\\\\notebook\\\\HomeworkMl\\\\mlclass-ex6-jin\\\\spamTrain.mat\")\n",
    "mat = ( datafile )\n",
    "X, y = mat['X'], mat['y']\n",
    "#NOT inserting a column of 1's in case SVM software does it for me automatically...\n",
    "#X =     np.insert(X    ,0,1,axis=1)\n",
    "\n",
    "# Test set\n",
    "datafile = loadmat('C:\\\\Users\\\\nomaniqbal\\\\Downloads\\\\notebook\\\\HomeworkMl\\\\mlclass-ex6-jin\\\\spamTest.mat')\n",
    "mat = ( datafile )\n",
    "Xtest, ytest = mat['Xtest'], mat['ytest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "495da1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training emails =  4000\n",
      "Number of training spam emails =  1277\n",
      "Number of training nonspam emails =  2723\n"
     ]
    }
   ],
   "source": [
    "pos = np.array([X[i] for i in range(X.shape[0]) if y[i] == 1])\n",
    "neg = np.array([X[i] for i in range(X.shape[0]) if y[i] == 0])\n",
    "print ('Total number of training emails = ',X.shape[0])\n",
    "print ('Number of training spam emails = ',pos.shape[0])\n",
    "print ('Number of training nonspam emails = ',neg.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9278a1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, kernel='linear')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the SVM training (with C = 0.1) using SVM software. \n",
    "\n",
    "# First we make an instance of an SVM with C=0.1 and 'linear' kernel\n",
    "linear_svm = svm.SVC(C=0.1, kernel='linear')\n",
    "\n",
    "# Now we fit the SVM to our X matrix, given the labels y\n",
    "linear_svm.fit( X, y.flatten() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93619692",
   "metadata": {},
   "source": [
    "# ===== Part 4: Test Spam Classification ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "227067b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy = 99.83%\n",
      "Test set accuracy = 98.90%\n"
     ]
    }
   ],
   "source": [
    "# \"Once the training completes, we should see that the classifier gets a \n",
    "#  training accuracy of about 99.8% and a test accuracy of about 98.5%\"\n",
    "\n",
    "train_predictions = linear_svm.predict(X).reshape((y.shape[0],1))\n",
    "train_acc = 100. * float(sum(train_predictions == y))/y.shape[0]\n",
    "print ('Training accuracy = %0.2f%%' % train_acc)\n",
    "\n",
    "test_predictions = linear_svm.predict(Xtest).reshape((ytest.shape[0],1))\n",
    "test_acc = 100. * float(sum(test_predictions == ytest))/ytest.shape[0]\n",
    "print ('Test set accuracy = %0.2f%%' % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a02818",
   "metadata": {},
   "source": [
    "# ===== Part 5: Top Predictors of Spam ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11b0647a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 15 most important words to classify a spam e-mail are:\n",
      "['otherwis', 'clearli', 'remot', 'gt', 'visa', 'base', 'doesn', 'wife', 'previous', 'player', 'mortgag', 'natur', 'll', 'futur', 'hot']\n",
      "\n",
      "The 15 least important words to classify a spam e-mail are:\n",
      "['http', 'toll', 'xp', 'ratio', 'august', 'unsubscrib', 'useless', 'numberth', 'round', 'linux', 'datapow', 'wrong', 'urgent', 'that', 'spam']\n",
      "\n",
      "# of spam containing \"otherwis\" = 804/1277 = 62.96%\n",
      "# of NON spam containing \"otherwis\" = 301/2723 = 11.05%\n"
     ]
    }
   ],
   "source": [
    "# Determine the words most likely to indicate an e-mail is a spam\n",
    "# From the trained SVM we can get a list of the weight coefficients for each\n",
    "# word (technically, each word index)\n",
    "\n",
    "vocab_dict_flipped = getVocabDict(reverse=True)\n",
    "\n",
    "#Sort indicies from most important to least-important (high to low weight)\n",
    "sorted_indices = np.argsort( linear_svm.coef_, axis=None )[::-1]\n",
    "print (\"The 15 most important words to classify a spam e-mail are:\")\n",
    "print ([ vocab_dict_flipped[x] for x in sorted_indices[:15] ])\n",
    "print()\n",
    "print (\"The 15 least important words to classify a spam e-mail are:\")\n",
    "print ([ vocab_dict_flipped[x] for x in sorted_indices[-15:] ])\n",
    "print()\n",
    "\n",
    "# Most common word (mostly to debug):\n",
    "most_common_word = vocab_dict_flipped[sorted_indices[0]]\n",
    "print ('# of spam containing \\\"%s\\\" = %d/%d = %0.2f%%'% \\\n",
    "    (most_common_word, sum(pos[:,1190]),pos.shape[0],  \\\n",
    "     100.*float(sum(pos[:,1190]))/pos.shape[0]))\n",
    "print ('# of NON spam containing \\\"%s\\\" = %d/%d = %0.2f%%'% \\\n",
    "    (most_common_word, sum(neg[:,1190]),neg.shape[0],      \\\n",
    "     100.*float(sum(neg[:,1190]))/neg.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505995a4",
   "metadata": {},
   "source": [
    "Note my SVM gets some different predictor words for spam than shown in the assignment PDF... I've done debugging and I'm confident it's due to a different SVM software package, not because of a bug or something in my code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
